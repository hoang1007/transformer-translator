{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from model import Translator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(config.SRC_MODEL_NAME)\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(config.TGT_MODEL_NAME)\n",
    "\n",
    "translator = Translator.load_from_checkpoint(\n",
    "    \"../checkpoints/epoch=0-step=16665.ckpt\",\n",
    "    src_tokenizer=src_tokenizer,\n",
    "    tgt_tokenizer=tgt_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mt_eng_vietnamese (/home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n",
      "Found cached dataset mt_eng_vietnamese (/home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n",
      "Loading cached processed dataset at /home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-9cf68e9f1ede0163.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8331c3ca39e6443e8b0d86e07ee2b47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133318 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-1f0f26e6f8f9de78.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2650203590e448a4bb6aae07e0129099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1269 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import Iwlst2015DataModule\n",
    "\n",
    "data_module = Iwlst2015DataModule(src_tokenizer, tgt_tokenizer, max_length=128, batch_size=4)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'src': {'input_ids': tensor([[  101,  1999,  5773,  1010,  2055,  2028,  1999,  2274,  2317,  7206,\n",
      "          2056,  1010,  1004, 22035,  2102,  1025,  2748,  1010,  2028,  1997,\n",
      "          1996,  2502,  4436,  2339,  1045,  5444,  2114, 13857,  8112,  2003,\n",
      "          2138,  2002,  2001,  2019,  3060,  1011,  2137,  1012,  1004, 22035,\n",
      "          2102,  1025,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,   101,  1998,\n",
      "          2002,  2106,  5247,  1037,  2843,  1997,  2051,  2039,  2000,  2053,\n",
      "          2204,  1012,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,   101,  2515,  2009,  2674,  2028,  3060,  2030,  1996,  2060,\n",
      "          3060,  1029,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           101,  2061,  1045,  2253,  2041,  1998,  1045,  4149,  1037,  2843,\n",
      "          1997, 16300,  2008,  2052,  1010,  1999,  2755,  1010,  8339,  2054,\n",
      "          1045,  2245,  2057,  2020,  2183,  2000,  2079,  2006,  2151,  2445,\n",
      "          2154,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'tgt': {'input_ids': tensor([[    0,   635, 27374,     4,   137,    99,    12,   100,   173,  2370,\n",
      "         14318,   482,   769,    96,    87,     4, 46787,  2845, 18240,    65,\n",
      "         14164,     4,    16,    12,    21,  3902,    91,   103,    64,    70,\n",
      "            17,  1083,    13,  3627, 12194,  2436,     8,    90,    46,   241,\n",
      "             8,    16,    18,    93,  1166,  4957,     5, 46787,  2845, 18240,\n",
      "            65,     2],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     0,\n",
      "           581,   287,     8,   881,    14,    47,    36,   129,    17,   118,\n",
      "             5,     2],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     0,  1244,    10,  8840,  8736,    15,\n",
      "            18,  2935,  4957,    23,   118,    18,  2935,  4957,  1357,    17,\n",
      "           114,     2],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     0,\n",
      "           218,    40,   227,     6,   188,    59,    36,   152, 26466,  5297,\n",
      "           184,    37,  2927,  7564,    38,  4623,   380,    70,   487,     8,\n",
      "            21,   148,   572,   237,    38,    47,    16,    43,   142,    37,\n",
      "             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}}\n"
     ]
    }
   ],
   "source": [
    "for batch in data_module.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def translate(text: str):\n",
    "    self = translator\n",
    "\n",
    "    src_token_ids, src_attention_mask = src_tokenizer(\n",
    "        text, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "    ).values()\n",
    "\n",
    "    src_token_ids = src_token_ids.to(self.device)\n",
    "    src_attention_mask = src_attention_mask.to(self.device)\n",
    "\n",
    "    print(src_token_ids, src_attention_mask)\n",
    "\n",
    "    tgt_token_ids = torch.tensor([[tgt_tokenizer.cls_token_id]], device=self.device)\n",
    "    tgt_attention_mask = torch.tensor([[1]], device=self.device)\n",
    "\n",
    "    for _ in range(config.MAX_SEQ_LEN):\n",
    "        logits = translator(\n",
    "            src_token_ids, tgt_token_ids, src_attention_mask, tgt_attention_mask\n",
    "        )\n",
    "\n",
    "        next_tgt_token_id = torch.argmax(logits[:, -1, :], keepdim=True, dim=-1)\n",
    "        tgt_token_ids = torch.cat([tgt_token_ids, next_tgt_token_id], dim=-1)\n",
    "        tgt_attention_mask = torch.cat(\n",
    "            [\n",
    "                tgt_attention_mask,\n",
    "                torch.ones_like(next_tgt_token_id, dtype=torch.int64)\n",
    "                if next_tgt_token_id != tgt_tokenizer.pad_token_id\n",
    "                else torch.zeros_like(next_tgt_token_id, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        if next_tgt_token_id == tgt_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    return tgt_tokenizer.decode(tgt_token_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2572, 1037, 3076,  102]]) tensor([[1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Cảm ơn. </s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I am a student\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
