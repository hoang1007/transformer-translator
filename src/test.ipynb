{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from model import Translator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(config.SRC_MODEL_NAME)\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(config.TGT_MODEL_NAME)\n",
    "\n",
    "translator = Translator.load_from_checkpoint(\n",
    "    \"../checkpoints/last.ckpt\",\n",
    "    src_tokenizer=src_tokenizer,\n",
    "    tgt_tokenizer=tgt_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def translate(text: str):\n",
    "    self = translator\n",
    "\n",
    "    src_token_ids, src_attention_mask = src_tokenizer(\n",
    "        text, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "    ).values()\n",
    "\n",
    "    src_token_ids = src_token_ids.to(self.device)\n",
    "    src_attention_mask = src_attention_mask.to(self.device)\n",
    "\n",
    "    print(src_token_ids, src_attention_mask)\n",
    "\n",
    "    tgt_token_ids = torch.tensor([[tgt_tokenizer.cls_token_id]], device=self.device)\n",
    "    tgt_attention_mask = torch.tensor([[1]], device=self.device)\n",
    "\n",
    "    for _ in range(config.MAX_SEQ_LEN):\n",
    "        logits = translator(\n",
    "            src_token_ids, tgt_token_ids, src_attention_mask, tgt_attention_mask\n",
    "        )\n",
    "\n",
    "        next_tgt_token_id = torch.argmax(logits[:, -1, :], keepdim=True, dim=-1)\n",
    "        tgt_token_ids = torch.cat([tgt_token_ids, next_tgt_token_id], dim=-1)\n",
    "        tgt_attention_mask = torch.cat(\n",
    "            [\n",
    "                tgt_attention_mask,\n",
    "                torch.ones_like(next_tgt_token_id, dtype=torch.int64)\n",
    "                if next_tgt_token_id != tgt_tokenizer.pad_token_id\n",
    "                else torch.zeros_like(next_tgt_token_id, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        if next_tgt_token_id == tgt_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    return tgt_tokenizer.decode(tgt_token_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self, key=lambda x: x, mode=\"min\"):\n",
    "        self.heap = []\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.key = lambda x: -key(x)\n",
    "        elif mode == \"min\":\n",
    "            self.key = key\n",
    "\n",
    "    def push(self, item):\n",
    "        heapq.heappush(self.heap, (self.key(item), item))\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.heap)[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n",
    "\n",
    "    def clear(self):\n",
    "        self.heap.clear()\n",
    "\n",
    "    def empty(self):\n",
    "        return len(self.heap) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_translate(text: str, max_translation_length: int = 50, beam_size: int = 3):\n",
    "    self = translator\n",
    "\n",
    "    src_token_ids, src_attention_mask = src_tokenizer(\n",
    "        text, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "    ).values()\n",
    "\n",
    "    src_token_ids = src_token_ids.to(self.device)\n",
    "    src_attention_mask = src_attention_mask.to(self.device)\n",
    "\n",
    "    tgt_token_ids = torch.tensor([[tgt_tokenizer.cls_token_id]], device=self.device)\n",
    "    tgt_attention_mask = torch.tensor([[1]], device=self.device)\n",
    "\n",
    "    # =========================== #\n",
    "    heap = PriorityQueue(key=lambda x: x[0], mode=\"min\")\n",
    "    heap.push((1.0, tgt_token_ids, tgt_attention_mask, False))\n",
    "\n",
    "    ret = []\n",
    "    for _ in range(max_translation_length):\n",
    "        # Keep track of the top k candidates\n",
    "        while len(heap) > beam_size:\n",
    "            heap.pop()\n",
    "\n",
    "        norm_prob = 0\n",
    "        mem = []\n",
    "\n",
    "        while not heap.empty():\n",
    "            # P(T1:Tn-1)\n",
    "            # tgt_token_ids.shape == (1, seq_len)\n",
    "            tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed = heap.pop()\n",
    "\n",
    "            if completed:\n",
    "                ret.append(\n",
    "                    (tgt_seq_prob.item(), tgt_tokenizer.decode(tgt_token_ids.squeeze_(), skip_special_tokens=True))\n",
    "                )\n",
    "\n",
    "                if len(ret) == beam_size:\n",
    "                    return ret\n",
    "                continue\n",
    "\n",
    "            norm_prob += tgt_seq_prob\n",
    "\n",
    "            logits = translator(\n",
    "                src_token_ids, tgt_token_ids, src_attention_mask, tgt_attention_mask\n",
    "            )\n",
    "\n",
    "            # (vocab_size,)\n",
    "            token_probs = torch.softmax(logits[:, -1, :], dim=-1).squeeze_()\n",
    "\n",
    "            # P(Tn | T1:Tn-1)\n",
    "            top_k_token_probs, top_k_token_ids = torch.topk(\n",
    "                token_probs, beam_size, largest=True\n",
    "            )\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                next_token_id = top_k_token_ids[i]\n",
    "                next_token_prob = top_k_token_probs[i]\n",
    "                completed = next_token_id == tgt_tokenizer.sep_token_id\n",
    "\n",
    "                mem.append(\n",
    "                    (\n",
    "                        tgt_seq_prob * next_token_prob,\n",
    "                        torch.cat((tgt_token_ids, next_token_id.view(1, 1)), dim=-1),\n",
    "                        torch.cat(\n",
    "                            (\n",
    "                                tgt_attention_mask,\n",
    "                                torch.tensor([[1]], device=self.device),\n",
    "                            ),\n",
    "                            dim=-1,\n",
    "                        ),\n",
    "                        completed,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed in mem:\n",
    "            tgt_seq_prob /= norm_prob  # normalize\n",
    "            heap.push((tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed))\n",
    "\n",
    "    while len(ret) < beam_size and not heap.empty():\n",
    "        tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed = heap.pop()\n",
    "\n",
    "        decoded_seq = tgt_tokenizer.decode(tgt_token_ids[0], skip_special_tokens=True)\n",
    "        ret.append((tgt_seq_prob.item(), decoded_seq))\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Khi tôi nhỏ.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.greedy_translate(\"When i was little\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21303218603134155,\n",
       "  'Tôi nhỏ, tôi nghĩ rằng đất nước mình là tốt nhất thế giới.'),\n",
       " (0.029819954186677933,\n",
       "  ', tôi nhỏ, tôi nghĩ rằng đất nước của mình là tốt nhất thế giới'),\n",
       " (0.3129540979862213,\n",
       "  ', tôi nhỏ, tôi nghĩ rằng đất nước mình là tốt nhất thế giới.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_translate(\"When I was little, I thought my country was the best in the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mt_eng_vietnamese (/home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "iwslt = load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-vi-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;',\n",
       " 'vi': 'Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt[0][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5021089911460876,\n",
       "  'Chúng tôi rất nhiều thời gian nghiên cứu rất nhiều thời gian nghiên cứu về lịch sử của Kim, ngoại trừ nước ngoài, ngoại trừ nước Mỹ, Nhật Bản là kẻ thù.'),\n",
       " (0.5036974549293518,\n",
       "  'Chúng tôi rất nhiều thời gian nghiên cứu rất nhiều thời gian nghiên cứu về lịch sử của Kim, ngoại trừ nước ngoài, ngoại trừ nước Mỹ, Nhật Bản là những kẻ thù.'),\n",
       " (0.03395974636077881,\n",
       "  'Chúng tôi rất nhiều thời gian nghiên cứu rất nhiều thời gian nghiên cứu về lịch sử của Kim, ngoại trừ nước ngoài, ngoại trừ nước Mỹ, Nhật Bản, Nhật Bản là kẻ thù địch')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_translate(iwslt[2][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt[2][\"translation\"][\"vi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ở, chúng tôi đã dành rất nhiều thời gian nghiên cứu về lịch sử của Joel, ngoại trừ nước ngoài, ngoại trừ nước Mỹ, ngoại trừ nước Mỹ, Nhật Bản là kẻ thù địch.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.greedy_translate(iwslt[2][\"translation\"][\"en\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
