{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/hoang/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from model import Translator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(config.SRC_MODEL_NAME)\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(config.TGT_MODEL_NAME)\n",
    "\n",
    "translator = Translator.load_from_checkpoint(\n",
    "    \"../checkpoints/last.ckpt\",\n",
    "    src_tokenizer=src_tokenizer,\n",
    "    tgt_tokenizer=tgt_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def translate(text: str):\n",
    "    self = translator\n",
    "\n",
    "    src_token_ids, src_attention_mask = src_tokenizer(\n",
    "        text, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "    ).values()\n",
    "\n",
    "    src_token_ids = src_token_ids.to(self.device)\n",
    "    src_attention_mask = src_attention_mask.to(self.device)\n",
    "\n",
    "    print(src_token_ids, src_attention_mask)\n",
    "\n",
    "    tgt_token_ids = torch.tensor([[tgt_tokenizer.cls_token_id]], device=self.device)\n",
    "    tgt_attention_mask = torch.tensor([[1]], device=self.device)\n",
    "\n",
    "    for _ in range(config.MAX_SEQ_LEN):\n",
    "        logits = translator(\n",
    "            src_token_ids, tgt_token_ids, src_attention_mask, tgt_attention_mask\n",
    "        )\n",
    "\n",
    "        next_tgt_token_id = torch.argmax(logits[:, -1, :], keepdim=True, dim=-1)\n",
    "        tgt_token_ids = torch.cat([tgt_token_ids, next_tgt_token_id], dim=-1)\n",
    "        tgt_attention_mask = torch.cat(\n",
    "            [\n",
    "                tgt_attention_mask,\n",
    "                torch.ones_like(next_tgt_token_id, dtype=torch.int64)\n",
    "                if next_tgt_token_id != tgt_tokenizer.pad_token_id\n",
    "                else torch.zeros_like(next_tgt_token_id, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        if next_tgt_token_id == tgt_tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    return tgt_tokenizer.decode(tgt_token_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self, key=lambda x: x, mode=\"min\"):\n",
    "        self.heap = []\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.key = lambda x: -key(x)\n",
    "        elif mode == \"min\":\n",
    "            self.key = key\n",
    "\n",
    "    def push(self, item):\n",
    "        heapq.heappush(self.heap, (self.key(item), item))\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.heap)[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n",
    "\n",
    "    def clear(self):\n",
    "        self.heap.clear()\n",
    "\n",
    "    def empty(self):\n",
    "        return len(self.heap) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(text: str, max_translation_length: int = 50, beam_size: int = 3):\n",
    "    self = translator\n",
    "\n",
    "    src_token_ids, src_attention_mask = src_tokenizer(\n",
    "        text, return_token_type_ids=False, return_tensors=\"pt\"\n",
    "    ).values()\n",
    "\n",
    "    src_token_ids = src_token_ids.to(self.device)\n",
    "    src_attention_mask = src_attention_mask.to(self.device)\n",
    "\n",
    "    tgt_token_ids = torch.tensor([[tgt_tokenizer.cls_token_id]], device=self.device)\n",
    "    tgt_attention_mask = torch.tensor([[1]], device=self.device)\n",
    "\n",
    "    # =========================== #\n",
    "    heap = PriorityQueue(key=lambda x: x[0], mode=\"min\")\n",
    "    heap.push((1.0, tgt_token_ids, tgt_attention_mask, False))\n",
    "\n",
    "    ret = []\n",
    "    for _ in range(max_translation_length):\n",
    "        # Completed\n",
    "        if len(ret) == beam_size:\n",
    "            break\n",
    "\n",
    "        # Keep track of the top k candidates\n",
    "        while len(heap) > beam_size:\n",
    "            heap.pop()\n",
    "\n",
    "        norm_prob = 0\n",
    "        mem = []\n",
    "\n",
    "        while not heap.empty():\n",
    "            # P(T1:Tn-1)\n",
    "            # tgt_token_ids.shape == (1, seq_len)\n",
    "            tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed = heap.pop()\n",
    "\n",
    "            if completed:\n",
    "                ret.append((tgt_seq_prob, tgt_tokenizer.decode(tgt_token_ids.squeeze_())))\n",
    "                continue\n",
    "\n",
    "            norm_prob += tgt_seq_prob\n",
    "\n",
    "            logits = translator(\n",
    "                src_token_ids, tgt_token_ids, src_attention_mask, tgt_attention_mask\n",
    "            )\n",
    "\n",
    "            # (vocab_size,)\n",
    "            token_probs = torch.softmax(logits[:, -1, :], dim=-1).squeeze_()\n",
    "\n",
    "            # P(Tn | T1:Tn-1)\n",
    "            top_k_token_probs, top_k_token_ids = torch.topk(\n",
    "                token_probs, beam_size, largest=True\n",
    "            )\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                next_token_id = top_k_token_ids[i]\n",
    "                next_token_prob = top_k_token_probs[i]\n",
    "                completed = next_token_id == tgt_tokenizer.sep_token_id\n",
    "\n",
    "                mem.append((\n",
    "                    tgt_seq_prob * next_token_prob,\n",
    "                    torch.cat((tgt_token_ids, next_token_id.view(1, 1)), dim=-1),\n",
    "                    torch.cat((tgt_attention_mask, torch.tensor([[1]], device=self.device)), dim=-1),\n",
    "                    completed\n",
    "                ))\n",
    "\n",
    "        for tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed in mem:\n",
    "            tgt_seq_prob /= norm_prob # normalize\n",
    "            heap.push((tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed))\n",
    "\n",
    "    while len(ret) < beam_size and not heap.empty():\n",
    "        tgt_seq_prob, tgt_token_ids, tgt_attention_mask, completed = heap.pop()\n",
    "        \n",
    "        decoded_seq = tgt_tokenizer.decode(tgt_token_ids[0])\n",
    "        ret.append((tgt_seq_prob, decoded_seq))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mt_eng_vietnamese (/home/hoang/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-vi-en/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "iwslt = load_dataset(\"mt_eng_vietnamese\", \"iwslt2015-vi-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;',\n",
       " 'vi': 'Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt[0][\"translation\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
